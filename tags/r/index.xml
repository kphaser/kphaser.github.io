<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Wong</title>
    <link>/tags/r/index.xml</link>
    <description>Recent content on Kevin Wong</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Kevin Wong</copyright>
    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Predicting Ames House Prices</title>
      <link>/post/predicting-ames-house-prices/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-ames-house-prices/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem we are trying to solve here is to build models to predict house prices, given the Ames Housing dataset, with high degree of predictive accuracy. The problem does not call for specific algorithms or techniques to be used. Just know that machine learning is a street brawl.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;significance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Significance&lt;/h3&gt;
&lt;p&gt;The goal of the problem was to utilize any and all machine learning tools or time series forecasting techniques to make the best possible prediction of house prices. This is an interesting problem because most people will eventually buy/sell a home. This problem allows us, as data scientists, to learn more about the housing market and helps with making more informed decisions.&lt;/p&gt;
&lt;p&gt;This project encouraged getting our hands dirty to clean, transform, and engineer features that enabled better predictions. This problem also necessitated learning popular algorithms and tools. The use of Kaggle as a platform for data science competition helped motivate us to keep improving.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Description&lt;/em&gt;&lt;br /&gt;
The Ames Housing dataset was retrieved from &lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data&lt;/a&gt;. The dataset represents residential properties in Ames, Iowa from 2006 to 2010. There is a train and a test file. The train file has 1460 observations and the test file has 1459 observations. Both datasets contain 79 explanatory variables composed of 46 categorical and 33 continuous variables that describe house features such as neighborhood, square footage, number of full bathrooms, and many more. The train file contains a response variable column, &lt;strong&gt;SalePrice&lt;/strong&gt;, which is what we will predict in the test set. There is also a unique ID for each house sold, but were not used in fitting the models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Exploratory Data Analysis&lt;/em&gt;&lt;br /&gt;
Below is a historgram of the SalePrice. Notice that the SalePrice is heavily skewed to the right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/saleprice-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/saleprice-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A log transformation was made to normalize the variable. This would allow algorithms such as linear regression, which rely on the assumption of linear relationships, to make better predictions.&lt;/p&gt;
&lt;p&gt;Next we examined some boxplots of certain variables against SalePrice. These help to confirm our understanding of what may affect a home’s sale price.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/neighborhoods-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that certain neighborhoods sell for more than others. The distribution of prices make sense here as there should be varying prices with certain neighborhoods clearly valued higher.&lt;/p&gt;
&lt;p&gt;Below are several more boxplots that indicate homes with more full bathrooms, finished garages, and central airconditioning tend to yield higher sale prices which is common sense.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can take a look at some numeric variables and their correlations to one another. Sometimes highly correlated variables may need to be removed. &lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/corrplot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above correlation matrix shows some variables that are highly correlated such as GrLivArea and TotRmsAbvGrd at 0.8 and GarageYrBlt and YearBuilt at 0.8. These variables were not be removed and were kept in the dataset as some of the models are fairly robust and correlated variables will not hinder predictive performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Preprocessing&lt;/em&gt;&lt;br /&gt;
The data preprocessing step proved to be the most crucial in this project. The train and test datasets were combined to keep the level of factors equal between the two sets so problems were not run into when predicting with unseen factor levels.&lt;/p&gt;
&lt;p&gt;In order to reduce the dimensionality of the dataset, the &lt;em&gt;nearZeroVar()&lt;/em&gt; function from the &lt;em&gt;caret&lt;/em&gt; package was used to remove variables that either have very few unique values. The purpose of removing these variables is because they do not add any additional information.&lt;/p&gt;
&lt;p&gt;All numeric variables that had a skewness of over 0.75 were transformed using the log function. The categorical variables were one hot encoded into binary variables which means that each factor level was given it’s own variable with values 0 or 1 representing no presence or presence of that feature.&lt;/p&gt;
&lt;p&gt;Certain variables contained many missing values. Any variables with over 20% missing were removed from the dataset. The remaining missing values were imputed using the &lt;em&gt;mice&lt;/em&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-of-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type of Models&lt;/h3&gt;
&lt;p&gt;The following are five models developed to predict house prices. These models were chosen because they showcase varying complexity and also have proven to perform well on many Kaggle problems.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;LASSO&lt;/strong&gt;&lt;br /&gt;
The LASSO, also known as Least Absolute Shrinkage and Selection Operator), is a regression model that does variable selection and regularization. The LASSO model uses a parameter that penalizes fitting too many variables. It allows the shrinkage of variable coefficients to 0, which essentially results in those variables having no effect in the model, thereby reducing dimensionality. Since there are quite a few explanatory variables, reducing the number of variables may increase interpretability and prediction accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;br /&gt;
A more advanced model, the random forest uses multiple decision trees and gives the mean prediction of each tree. This is somewhat of a black box approach as the random forest mechanism is not very clear as it will give model results, but lack information on coefficients which is something we normally get from an output of a regression model. However, the random forest model is a great general purpose algorithm that has potential to make quite accurate predictions. Random forests can be quite robusts against outliers and do not require assumptions of normality.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GBM&lt;/strong&gt; Gradient boosting models are one of the most popular algorithms on Kaggle. A variant of GBMs known as the XGBoost has been a clear favorite for many recent competitions. The algorithm works well right out of the box. It is a type of ensemble model, like the random forest, where multiple decision trees are used and optimized over some cost function. The popularity and ability to score well in competition are reasons enough to use this type of model for house price prediction problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt;&lt;br /&gt;
Neural nets have recently been popularized in the media especially for their use in deep learning. Neural networks mimic the function of a human brain. There are multiple layers containing neurons. Inputs into each layer become the output for the subsequent layer until there are no additional layers. Neural networks are useful in this problem because they can take complex datasets and find patterns. This model can be useful when dealing with mixed variable type datasets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ensemble Model/Model Averaging&lt;/strong&gt;&lt;br /&gt;
The last type of model used was the ensemble model, specifically model averaging. Several models are created that perform relaively well on the problem and the predictions are averaged. In this case, all the models above were used to create an average prediction. The reason for using this model was that the recent Kaggle winners used ensemble models. It is not surprising that most winning submissions are ensembles because one can leverage the prediction power of several different models to obtain the highest prediction accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Literature&lt;/h3&gt;
&lt;p&gt;The following five examples refer to the type of models used in this report that have been applied to similar cases.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LASSO&lt;/em&gt;&lt;br /&gt;
In Taiwan, head and neck cancers are one of the leading causes of mortality. One of the common treatments is radiotherapy which often results in xerostomia which is severe dryness of the mouth. Several researchers analyzed questionnaires from 206 patients before, during and after radiotherapy. The reseachers analyzed the variables 3 and 12 months post radiotherapy. The LASSO model was used to rank and select strongly correlated prognostic factors based on the AUC. The LASSO selected eight factors 3 months after radiotherapy and nine factors 12 months after radiotherapy. The LASSO model had an AUC score of 0.86 and 0.87 for the post 3 and 12 months after radiotherapy respectively. (Lee, Chao, Ting, Chang, Huang, Wu, &amp;amp; Leung, 2014)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Random Forest&lt;/em&gt;&lt;br /&gt;
Flavonoids are an important research objective when it comes to wine because they affect the sensory quality of fruits and vegetables. When berries age and ripen the skin texture changes and affects the sensory quality of the grapes. Measuring flavinoids is time consuming so a group of researchers employed machine learning algorithms to try and accurately predict skin flavonoid content. They used several ensemble methods including random forest. They took texture properties and flavonoid content measured in 22 wine grapes. They were able to obtain high accuracy with &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; of 0.836 and RMSE of 0.729 on the test set. The result is a greater exposure and appreciation for machine learning in the area of chemometrics (Brillante, Gaiotti, Lovat, Vincenzi, Giacosa, Torchio, Segade, Rolle, &amp;amp; Tomasi, 2015).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;GBM&lt;/em&gt;&lt;br /&gt;
An interesting application of GBMs is for auto insurance loss cost prediction. GLMs are generally an accepted framework for insurance pricing models, however the exploration into GBMs, as it is a very robust method, was attractive to a group of auto insurance researchers. The performance of the GBM was compared with the GLM by calculating a ratio of the rate the insurance company would charge based on the GBM model to the rate the insurance company would charge based on the GLM model resulting in a GLM-loss ratio. It was found that the GLM loss-ratio increases when the GBM would suggest charger higher rate than GLM which indicates the GBM had higher predictive performance (Guelman, 2012).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Neural Networks&lt;/em&gt;&lt;br /&gt;
Neural networks have been used to solve all sorts of complex and unique problems. One of those problems is predicting energy consumption and thermal comfort level of a swimming pool. The neural network was trained and optimized in real conditions in order to achieve energy cost savings. Variables such as hour, day, month, humidity, water temperature, room temperature, and supplied flow rate were inputs into the neural network model with outputs being electricity consumption, thermal energy consumption, and thermal comfort (PMV). The results of testing error rate allowed the researchers to determine the optimal hidden layers and epochs which is similar to our use of cross-validation techniques on the neural network later in this report. The use of neural networks here allows for creation of intelligent swimming pool systems (Yuce, Li, Rezgui, Petri, Jayan, &amp;amp; Yang, 2014).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ensemble Model/Model Averaging&lt;/em&gt;&lt;br /&gt;
A study of the Pacific Northwest region runoff shows that multi-model ensemble outperforms single model predictions. The ensemble model was used to predict projected seasonal runoff extremes compared to historical simulations. Several models were created and combined through the use of Bayesian model averaging and then made to predict the runoff extremes for each season. The results were compared with the observed historical simulation on a map grid. The ensemble models predicted that there would be an increase in extreme runoff in the future in most of the Pacific Northwest regions (Najafi &amp;amp; Moradkhani, 2015).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;formulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formulation&lt;/h3&gt;
&lt;p&gt;After processing the dataset, the training was separated into training and validation sets using a 75/25 split. In this way, models were fit on the training and evaluated or tuned on validation.&lt;/p&gt;
&lt;p&gt;The models were implemented using the &lt;em&gt;caret&lt;/em&gt; and &lt;em&gt;h2o&lt;/em&gt; packages. The caret package was used primarily for parameter tuning. The package allows grid search where you specify a data frame of parameter values. It also works with hundreds of popular machine learning algorithms. The models were fit using repeated 5-fold cross-validation. An example of my usage is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitControl &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;,
                           number=5,
                           repeats=5,
                           verboseIter=FALSE)


set.seed(123)  # for reproducibility
lasso_mod &amp;lt;- train(x=x_train,y=y_train,
                   method=&amp;quot;glmnet&amp;quot;,
                   metric=&amp;quot;RMSE&amp;quot;,
                   maximize=FALSE,
                   trControl=fitControl,
                   tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                        lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                                 0.00075,0.0005,0.0001)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Models can take fairly long to run even with a dataset of this size (2919 total observations and 80+ variables). I discovered the &lt;em&gt;h2o&lt;/em&gt; package which allows R to run in-memory, in distributed fashion. This sped up many models that I had already built by at least 20x running on my local machine. Once you load the h2o package and initiate the cluster, you can fit a model like a neural network very fast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dl_fit &amp;lt;- h2o.deeplearning(x = x,
                            y = z,
                            training_frame = train_h2o,
                            model_id = &amp;quot;dl_fit2&amp;quot;,
                            # validation_frame = valid_h2o,  # only used if stopping_rounds &amp;gt; 0
                            epochs = 20,
                            hidden= c(10,10),
                            stopping_rounds = 0,  # disable early stopping
                            seed = 1)

dl_perf &amp;lt;- h2o.performance(model = dl_fit2, newdata = valid_h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the models were iterated over several variations of parameters and data preprocessing situations in order to obtain the best possible predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Performance&lt;/h3&gt;
&lt;p&gt;Each model was compared using the Log RMSE on a validation set. The Kaggle public score shows the results on the public test set. As of writing, the competition has not ended, so there are no results on the private heldout test set.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Cross Validation Log RMSE&lt;/th&gt;
&lt;th&gt;Kaggle Public Log RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;LASSO&lt;/td&gt;
&lt;td&gt;0.12783&lt;/td&gt;
&lt;td&gt;0.12390&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;0.12567&lt;/td&gt;
&lt;td&gt;0.14043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;GBM&lt;/td&gt;
&lt;td&gt;0.12382&lt;/td&gt;
&lt;td&gt;0.13733&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Neural Network&lt;/td&gt;
&lt;td&gt;0.14334&lt;/td&gt;
&lt;td&gt;0.16168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Ensemble (GLM+GBM)&lt;/td&gt;
&lt;td&gt;0.10757&lt;/td&gt;
&lt;td&gt;0.12523&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model that performed the best was the LASSO model. It did not have the best cross validation score, but the best public leaderboard score on Kaggle. The LASSO model is more easily interpretable model than the others since it reduces the dimensionality of the dataset by shrinking variables to 0. It allows for a much more manageable set of variables to work with. It makes sense that the LASSO model performed well because many variable appeared to have some linear relationship with the reponse variable, SalePrice.&lt;/p&gt;
&lt;p&gt;The next best model was the ensemble model. By leveraging multiple models, the ensemble model was able to obtain the pretty good prediction accuracy. However, brute force and adding multiple models and averaging their predictions did not prove to be the best model. Surprisingly, the Neural Network model did not perform well at all. Even with parameter tuning, it never was able to beat any of the other models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;p&gt;The Ensemble model has some clear limitations. It is not at all easily interpretable despite yielding high prediction accuracy. The Ensemble model requires brute forcing multiple models which can make things complicated fast if you are not careful. It also heavily relies on the models that go into it.&lt;/p&gt;
&lt;p&gt;The Random Forest, GBM, and Neural Network are fairly robust models that can work on a variety of classification and regression problems. Much of the limitaions of these models relies on the tuning of the parameters and the inputs to the model. The same goes for the LASSO model where one can tune the lambda values. The LASSO is more reliant on the assumption that the explanatory variables are linearly related to the response.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Future Work&lt;/h3&gt;
&lt;p&gt;Some future considerations for this problem may be to perform more feature engineering. There are likely other variables that can provide more information. The models can be further developed through more hyper-parameterization and perhaps running them in a distributed computing environment.&lt;/p&gt;
&lt;p&gt;The goal in developing these models is to be able to better understand and to apply them to other real world datasets. I am interested into taking some of these models to more complex problems like image classification and building some interactive applications around these models. My hope is to be able to showcase this project and other future similar projects to employers/clients. This problem has also inspired me to compete in other Kaggle competitions using models developed here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Learning&lt;/h3&gt;
&lt;p&gt;This project was fairly complex in terms of dealing with many explanatory variables. By no means was this big data, but higher dimension datasets can be difficult to deal with if not prepared appropriately to be fed into machine learning algorithms.&lt;/p&gt;
&lt;p&gt;Some of the major takeaways from this project are that there are packages that help automate the modeling process like the &lt;em&gt;caret&lt;/em&gt; package. This package allows you to utilize grid search and cross validation to select optimal model parameters. This was especially helpful when fine tuning models to make better predictions.&lt;/p&gt;
&lt;p&gt;Another nice package/engine is the &lt;em&gt;h2o&lt;/em&gt; package. It allows you to run machine learning algorithms in-memory in a distributed fashion. So for algorithms like neural networks or random forests, which can take a long time to train, would literally take a few seconds. This allows more iterations and experimentation.&lt;/p&gt;
&lt;p&gt;Model complexity does not necessarily lead to better predictions. As mentioned earlier, simpler models like the LASSO, could produce better results than black box methods like neural networks, which did not seem to perform well in this competition.&lt;/p&gt;
&lt;p&gt;Lastly, because the dataset contained many variables, it really challenged my skills in R in terms of manipulating data and writing functions more effectively and efficiently.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Brillante, Luca, Federica Gaiotti, Lorenzo Lovat, Simone Vincenzi, Simone Giacosa, Fabrizio Torchio, Susana Río Segade, Luca Rolle, &amp;amp; Diego Tomasi. “Investigating the Use of Gradient Boosting Machine, Random Forest and Their Ensemble to Predict Skin Flavonoid Content from Berry Physical–mechanical Characteristics in Wine Grapes.” Computers and Electronics in Agriculture 117 (2015): 186-93. Web.&lt;/p&gt;
&lt;p&gt;Guelman, L. (2012). Gradient boosting trees for auto insurance loss cost modeling and prediction. Expert Systems with Applications, 39(3), 3659-3667. &lt;a href=&#34;doi:10.1016/j.eswa.2011.09.058&#34; class=&#34;uri&#34;&gt;doi:10.1016/j.eswa.2011.09.058&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lee, T.-F., Chao, P.-J., Ting, H.-M., Chang, L., Huang, Y.-J., Wu, J.-M., … Leung, S. W. (2014). Using Multivariate Regression Model with Least Absolute Shrinkage and Selection Operator (LASSO) to Predict the Incidence of Xerostomia after Intensity-Modulated Radiotherapy for Head and Neck Cancer. PLoS ONE, 9(2), e89700. &lt;a href=&#34;http://doi.org/10.1371/journal.pone.0089700&#34; class=&#34;uri&#34;&gt;http://doi.org/10.1371/journal.pone.0089700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Najafi, M. R., &amp;amp; Moradkhani, H. (2015). Multi-model ensemble analysis of runoff extremes for climate change impact assessments. Journal of Hydrology, 525, 352-361. &lt;a href=&#34;doi:10.1016/j.jhydrol.2015.03.045&#34; class=&#34;uri&#34;&gt;doi:10.1016/j.jhydrol.2015.03.045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yuce, B., Li, H., Rezgui, Y., Petri, I., Jayan, B., &amp;amp; Yang, C. (2014). Utilizing artificial neural network to predict energy consumption and thermal comfort level: An indoor swimming pool case study. Energy and Buildings, 80, 45-56. &lt;a href=&#34;doi:10.1016/j.enbuild.2014.04.052&#34; class=&#34;uri&#34;&gt;doi:10.1016/j.enbuild.2014.04.052&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    script.src  = &#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;;
    if (location.protocol !== &#34;file:&#34; &amp;&amp; /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, &#39;&#39;);
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;

&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Blogging with R Markdown</title>
      <link>/post/blogging-with-r-markdown/</link>
      <pubDate>Mon, 05 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/blogging-with-r-markdown/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt;Throughout my graduate studies, I’ve used R quite a bit and have grown fond of it. When it comes to doing data science and machine learning, R fits the bill nicely. Not to mention R has a huge following and is currently one of the most popular languages in the world. As a result, it has a very active community and a huge selection of packages that pretty much allows R to do anything you want like reproducible reporting.&lt;/p&gt;
&lt;p&gt;One of the key skills of a data scientist is to communicate and share your analyses/models with your peers or stakeholders. If you can’t effectively show what you did, then what’s the point? The guys at RStudio have created a great solution called R Markdown. If you’ve ever used Markdown, then you know how easy it is. R Markdown allows one to embed R code into a reporting document. Just start a new .Rmd file in RStudio and adjust the YAML front matter at the very top of the document and you’re good to go. R Markdown borrows the syntax from vanilla Markdown where most of the difference is in parameterizing the code chunks. R Markdown is amazing for reproducibility. Also, anyone that’s ever seen a R Markdown document knows it’s one of the prettier looking documents. R Markdown even has the capability to embed Shiny apps/interactive widgets.&lt;/p&gt;
&lt;p&gt;However, there is one drawback to R Markdown right now and that’s blogging. In an ideal world, I would be in RStudio, writing a post in R Markdown and pushing it to github pages and be done with it. But it’s not there yet. I’ve seen many solutions from using Jekyll (which I haven’t found to be easy) to spinning up your own AWS EC2 instance and setting up a blog there to host html files. Most seem like more work than it should be just to put up a nice looking R Markdown page. After much searching around, I found &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt; by Yihui Xie, the man who created Knitr. Blogdown uses &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;, a blogging framework for static pages built using the &lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt; language. One word of warning, blogdown is still under development, but I’ve tested it out and there have been minor issues, but most of it is because there is no documentation right now if you’re stuck. I’ve had to go under the hood and look at the code to figure out what functions to run. Below I’ll show a quick way to get set up so that blogging with R Markdown is joy from start to finish.&lt;/p&gt;
&lt;p&gt;Assuming you already have R/RStudio installed, you’ll need to download the blogdown package to your machine like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;#39;rstudio/blogdown&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, you’ll need to install Hugo to your local machine using the blogdown function &lt;code&gt;install_hugo()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blogdown::install_hugo()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can create a new directory for our webpage and make sure to set your working directory to that new directory. Once in your empty directory, run the following command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blogdown::new_site()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the above does not work, make sure you are creating a completely empty directory. I ran into a few issues when I tried to run the above function on a directory with a .Rproj file in it. So remember, the directory will need to be &lt;em&gt;totally empty&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The above command will generate all the necessary skeleton files needed to generate the blog and launch it in the console. It will also show a preview of your blog in the viewer window in RStudio. The above command also automatically downloads a theme, but this can easily be changed if you choose with the below command. You can find more Hugo themes at &lt;a href=&#34;themes.gohugo.io&#34;&gt;here&lt;/a&gt;. My current theme is academic. You can change it to whatever you want by specifying the github repo and the theme name as shown below in quotes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blogdown::install_theme(&amp;#39;gcushen/hugo-academic&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind when you install a new theme, the config file &lt;code&gt;config.toml&lt;/code&gt; will update to use the most recently downloaded theme as default. So to change it in the future just go into the &lt;code&gt;config.toml&lt;/code&gt; file and adjust the &lt;code&gt;theme =&lt;/code&gt; parameter to the name of the theme you want.&lt;/p&gt;
&lt;p&gt;When you’re happy with the theme, it’s good practice to preview your blog before pushing it out to the world. Run the following in your console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blogdown::serve_site()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate the blog and preview it on your local machine. Blogdown loads your website on your localhost IP address. Mine shows up here: &lt;code&gt;http://127.0.0.1:4321/&lt;/code&gt;. If you’re using RStudio, you’ll be able to preview the site/post and make changes on the fly. When you make changes to the R Markdown document being previewed, blogdown will rebuild the website when you hit save and the Viewer in RStudio will automatically show the updated content. You can also use &lt;code&gt;blogdown::build_site()&lt;/code&gt; to build the site for publishing, but it will not preview it. Hugo generates the published content in the &lt;code&gt;public/&lt;/code&gt; directory of your root.&lt;/p&gt;
&lt;p&gt;When you create a new Rmd file in RStudio, a date will automatically be propogated for you in the &lt;code&gt;&amp;quot;December 5, 2016&amp;quot;&lt;/code&gt; format in the YAML front matter. Hugo will not be able to correctly parse this date when it builds the website and will throw an error. So make sure to manually change the date to this format &lt;code&gt;&amp;quot;2016-12-06&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you followed those basic steps then you should have the website files in the &lt;code&gt;public/&lt;/code&gt; directory and you’re good to go. I won’t show you how to put your blog posts online as everyone has their preference for blogging platform. I personally use github pages because it’s free and having version control with your blog posts is nice. Blogdown is a good complement because I don’t have to worry converting &lt;code&gt;.Rmd&lt;/code&gt; files to &lt;code&gt;.md&lt;/code&gt; files and put plot figures in separate folders; it’s all done for me.&lt;/p&gt;
&lt;p&gt;Remember, blogdown is still under development so many people are bound to run into errors and the lack of documentation doesn’t help. To their credit, the RStudio guys are busy making our lives better. Blogdown is a great start to easy blogging in R Markdown. And I look forward to more developments coming to this package.&lt;/p&gt;
&lt;p&gt;I hope this post helped some of you out there like me who love R Markdown and want to easily share these type of documents. Happy R Markdown blogging!&lt;/p&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Scraping NBA Stats From The Web</title>
      <link>/post/web-scraping-with-r/</link>
      <pubDate>Sat, 11 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/web-scraping-with-r/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt;Web scraping using R is a fairly easy task. Let’s take a look at the Coach of the Year statistics from the ESPN.com site. First install the necessary packages. &lt;code&gt;rvest&lt;/code&gt; allows you to extract data from a webpage. &lt;code&gt;stringr&lt;/code&gt; allows you to manipulate strings. &lt;code&gt;tidyr&lt;/code&gt; will load the data manipulation libraries that’ll be useful for selecting and munging data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;rvest&amp;#39;)
install.packages(&amp;#39;stringr&amp;#39;)
install.packages(&amp;#39;tidyr&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add an extra library here because &lt;code&gt;html_nodes()&lt;/code&gt; function fails when not run interactively. Load the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(stringr)
library(tidyr)
library(methods)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read the wepage in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;#39;http://www.espn.com/nba/history/awards/_/id/34&amp;#39;
site &amp;lt;- read_html(url)
coach_table &amp;lt;- html_nodes(site, &amp;#39;table&amp;#39;)
coaches &amp;lt;- html_table(coach_table,fill=TRUE)[[1]]
head(coaches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  X1                X2                    X3
## 1 Coach of the Year Coach of the Year     Coach of the Year
## 2              YEAR             COACH                  TEAM
## 3              2016        Steve Kerr Golden State Warriors
## 4              2015  Mike Budenholzer         Atlanta Hawks
## 5              2014    Gregg Popovich     San Antonio Spurs
## 6              2013       George Karl        Denver Nuggets
##                  X4                X5                X6                X7
## 1 Coach of the Year Coach of the Year Coach of the Year Coach of the Year
## 2               W-L      PLAYOFFS W-L        CAREER W-L               EXP
## 3              73-9              15-9            161-28           2 years
## 4             60-22               8-8           158-112           3 years
## 5             62-20              16-7          1108-490          20 years
## 6             57-25               2-4          1175-824          27 years
##                  X8                X9               X10
## 1 Coach of the Year Coach of the Year Coach of the Year
## 2              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;
## 3              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;
## 4              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;
## 5              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;
## 6              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;              &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are extra rows and columns we don’t need so we’ll remove them and give names to the remaining columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coaches &amp;lt;- coaches[-(1:2), -(8:10)]
names(coaches) &amp;lt;- c(&amp;quot;year&amp;quot;, &amp;quot;coach&amp;quot;, &amp;quot;team&amp;quot;, &amp;quot;season_record&amp;quot;,&amp;quot;playoff_record&amp;quot;,&amp;quot;career_record&amp;quot;,&amp;quot;experience&amp;quot;)
coaches$year &amp;lt;- as.integer(coaches$year)
head(coaches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   year            coach                  team season_record playoff_record
## 3 2016       Steve Kerr Golden State Warriors          73-9           15-9
## 4 2015 Mike Budenholzer         Atlanta Hawks         60-22            8-8
## 5 2014   Gregg Popovich     San Antonio Spurs         62-20           16-7
## 6 2013      George Karl        Denver Nuggets         57-25            2-4
## 7 2012   Gregg Popovich     San Antonio Spurs         50-16           10-4
## 8 2011    Tom Thibodeau         Chicago Bulls         62-20            9-7
##   career_record experience
## 3        161-28    2 years
## 4       158-112    3 years
## 5      1108-490   20 years
## 6      1175-824   27 years
## 7      1108-490   20 years
## 8       261-157    2 years&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll want to split up some columns and convert them to integers. Those are the &lt;code&gt;season_record&lt;/code&gt;, &lt;code&gt;playoff_record&lt;/code&gt;, and &lt;code&gt;career_record&lt;/code&gt; columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coaches &amp;lt;- separate(coaches, season_record, c(&amp;#39;season_wins&amp;#39;, &amp;#39;season_losses&amp;#39;), sep=&amp;#39;-&amp;#39;, remove=TRUE, convert=TRUE)
head(coaches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   year            coach                  team season_wins season_losses
## 3 2016       Steve Kerr Golden State Warriors          73             9
## 4 2015 Mike Budenholzer         Atlanta Hawks          60            22
## 5 2014   Gregg Popovich     San Antonio Spurs          62            20
## 6 2013      George Karl        Denver Nuggets          57            25
## 7 2012   Gregg Popovich     San Antonio Spurs          50            16
## 8 2011    Tom Thibodeau         Chicago Bulls          62            20
##   playoff_record career_record experience
## 3           15-9        161-28    2 years
## 4            8-8       158-112    3 years
## 5           16-7      1108-490   20 years
## 6            2-4      1175-824   27 years
## 7           10-4      1108-490   20 years
## 8            9-7       261-157    2 years&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s do the same for &lt;code&gt;playoff_record&lt;/code&gt; and &lt;code&gt;career_record&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coaches &amp;lt;- separate(coaches, playoff_record, c(&amp;#39;playoff_wins&amp;#39;, &amp;#39;playoff_losses&amp;#39;), sep=&amp;#39;-&amp;#39;, remove=TRUE, convert=TRUE)
coaches &amp;lt;- separate(coaches, career_record, c(&amp;#39;career_wins&amp;#39;, &amp;#39;career_losses&amp;#39;), sep=&amp;#39;-&amp;#39;, remove=TRUE, convert=TRUE)
head(coaches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   year            coach                  team season_wins season_losses
## 3 2016       Steve Kerr Golden State Warriors          73             9
## 4 2015 Mike Budenholzer         Atlanta Hawks          60            22
## 5 2014   Gregg Popovich     San Antonio Spurs          62            20
## 6 2013      George Karl        Denver Nuggets          57            25
## 7 2012   Gregg Popovich     San Antonio Spurs          50            16
## 8 2011    Tom Thibodeau         Chicago Bulls          62            20
##   playoff_wins playoff_losses career_wins career_losses experience
## 3           15              9         161            28    2 years
## 4            8              8         158           112    3 years
## 5           16              7        1108           490   20 years
## 6            2              4        1175           824   27 years
## 7           10              4        1108           490   20 years
## 8            9              7         261           157    2 years&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, remove the character &lt;code&gt;years&lt;/code&gt; from the experience column and make it an integer. But hold on, there’s actually an easier way. We can use the &lt;code&gt;extract_numeric()&lt;/code&gt; function (update: this function has been deprecated) to get the numbers and remove the old column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coaches$yrs_exp &amp;lt;- as.integer(extract_numeric(coaches$experience))
coaches$experience &amp;lt;- NULL
head(coaches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   year            coach                  team season_wins season_losses
## 3 2016       Steve Kerr Golden State Warriors          73             9
## 4 2015 Mike Budenholzer         Atlanta Hawks          60            22
## 5 2014   Gregg Popovich     San Antonio Spurs          62            20
## 6 2013      George Karl        Denver Nuggets          57            25
## 7 2012   Gregg Popovich     San Antonio Spurs          50            16
## 8 2011    Tom Thibodeau         Chicago Bulls          62            20
##   playoff_wins playoff_losses career_wins career_losses yrs_exp
## 3           15              9         161            28       2
## 4            8              8         158           112       3
## 5           16              7        1108           490      20
## 6            2              4        1175           824      27
## 7           10              4        1108           490      20
## 8            9              7         261           157       2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we have it! We have a tidy data frame in which we can do some analysis now! For example, we can figure out which team has the most Coach of the Years using the &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
coaches %&amp;gt;% select(team) %&amp;gt;% ggplot(aes(team), fill=team) + geom_bar() + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/web-scraping-with-r_files/figure-html/ggplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So it looks like the Atlanta Hawks and the Chicago Bulls have had some pretty coaches throughout NBA history. There’s much more analysis that can be done with this dataset. We can ask questions like which coach has the most wins all-time? Or we can try to make interesting connections like, do years of experience correlate with more wins? But I’ll leave that to you to find out!&lt;/p&gt;
&lt;p&gt;It’s a good idea to save your data frame to use later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(coaches, &amp;#39;coaches.csv&amp;#39;, row.names=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other considerations when it comes to scraping and parsing web data like missing values. We can take a look at more of the data and notice some NA values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(coaches, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    year            coach                   team season_wins season_losses
## 3  2016       Steve Kerr  Golden State Warriors          73             9
## 4  2015 Mike Budenholzer          Atlanta Hawks          60            22
## 5  2014   Gregg Popovich      San Antonio Spurs          62            20
## 6  2013      George Karl         Denver Nuggets          57            25
## 7  2012   Gregg Popovich      San Antonio Spurs          50            16
## 8  2011    Tom Thibodeau          Chicago Bulls          62            20
## 9  2010     Scott Brooks  Oklahoma City Thunder          50            32
## 10 2009       Mike Brown    Cleveland Cavaliers          66            16
## 11 2008      Byron Scott    New Orleans Hornets          56            26
## 12 2007     Sam Mitchell        Toronto Raptors          47            35
## 13 2006    Avery Johnson       Dallas Mavericks          60            22
## 14 2005    Mike D&amp;#39;Antoni           Phoenix Suns          62            20
## 15 2004      Hubie Brown      Memphis Grizzlies          50            32
## 16 2003      Hubie Brown      Memphis Grizzlies          28            41
## 17   NA   Gregg Popovich      San Antonio Spurs          60            22
## 18 2002    Rick Carlisle        Detroit Pistons          50            32
## 19 2001      Larry Brown     Philadelphia 76ers          56            26
## 20 2000       Doc Rivers          Orlando Magic          41            41
## 21 1999    Mike Dunleavy Portland Trail Blazers          35            15
## 22 1998       Larry Bird         Indiana Pacers          58            24
##    playoff_wins playoff_losses career_wins career_losses yrs_exp
## 3            15              9         161            28       2
## 4             8              8         158           112       3
## 5            16              7        1108           490      20
## 6             2              4        1175           824      27
## 7            10              4        1108           490      20
## 8             9              7         261           157       2
## 9             2              4         347           220       0
## 10           10              4         347           216       0
## 11            7              5         454           647      15
## 12            2              4         185           243       6
## 13           14              9         254           186       6
## 14            9              6         472           433      12
## 15            0              4         424           489      13
## 16           NA             NA         424           489      13
## 17           16              8        1108           490      20
## 18            4              6         666           489      14
## 19           12             11        1098           904       0
## 20           NA             NA         770           560      17
## 21            7              6         613           716      17
## 22           10              6         147            67       3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this is a small dataset, we could infer what those missing values could be. For example, there is a missing value in &lt;code&gt;year&lt;/code&gt; of row 17, which is obvious it should be &lt;code&gt;2002&lt;/code&gt;. So we could manually impute that value.&lt;/p&gt;
&lt;p&gt;There are NAs in the &lt;code&gt;playoff_wins&lt;/code&gt; and &lt;code&gt;playoff_losses&lt;/code&gt; columns. Those are a little trickier and one could possibly impute with more complex methods. But if you have some domain knowledge in sports, you know it’s possible for coaches to win Coach of the Year, but not make it to the playoffs hence the NA values under those columns. So it might be sensible to manually impute with &lt;code&gt;0&lt;/code&gt; or some other value. Although that’s a whole another discussion about imputation techniques and feature engineering which I’ll try to cover in another blog post. Hope that was a quick and helpful introduction to pulling data from the web in R!&lt;/p&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Data Science Tools</title>
      <link>/post/data-science-tools/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/data-science-tools/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt;One of the most talked about things in the data science community is tooling. What is the best programming language to learn? What IDE should one use? There is no one definitive answer here, but the general answer largely depends on what you’re trying to accomplish and what features you care about and what your workflow is like.&lt;/p&gt;
&lt;p&gt;When I started learning data science, I was overwhelmed by how many tools were out there. Trying to learn every single one is a rather ineffective approach from personal experience. I’ve since focused on mastering one language and one tool and it has helped my understanding of data science and workflow productivity tremendously. That’s not to say you shouldn’t learn others when necessary, but my preference is to be the “master of one” than the “jack of all trades who is the master of none”. Mastering one language and one tool will make learning others so much easier in the long run. But that’s besides the point.&lt;/p&gt;
&lt;p&gt;Below is a list of tools that I prefer when working with a particular language or working in a specific environment, but there are many more out there:&lt;/p&gt;
&lt;div id=&#34;r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RStudio: quintessential IDE for working in R. I use this everyday.&lt;/li&gt;
&lt;li&gt;Jupyter Notebooks: I tend to use this when I’m working in R on a server. This is also great for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Anaconda: great Python distribution installer that comes with all the tools like Jupyter Notebooks and data science libraries.&lt;/li&gt;
&lt;li&gt;PyCharm: strong IDE for specifically working in Python. Great debugging features.&lt;/li&gt;
&lt;li&gt;Enthought Canopy: An alternative to Anaconda.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;sas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SAS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SAS Studio: This is what I used in grad school so I’ve stuck with it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;unix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cmder / Putty: for ssh’ing to servers using Windows because the Windows Command Prompt is just bad.&lt;/li&gt;
&lt;li&gt;Vim: There’s a whole debate about Vim vs. Emacs. I prefer Vim for quick text editing. Emacs is an ecosystem in it of itself.&lt;/li&gt;
&lt;li&gt;Git: not much to say here. It’s necessary to have good understanding of version control when working with code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft OneNote: awesome for taking notes and creating pages. Similar to Evernote, you can also paste graphics in it. Widely available in most OS now.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-viz&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Viz&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ggplot2: my go to because the graphics are just beautiful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;other&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ProjectTemplate: a package in &lt;code&gt;R&lt;/code&gt; that creates a system of directories specific for different tasks like data munging, reporting, etc. All you have to do is run &lt;code&gt;load.project()&lt;/code&gt; and it’ll load all your dependencies, files, and prepared data so you can focus on your next steps of analysis/modeling. It’s awesome! You can check it out &lt;a href=&#34;www.projecttemplate.net&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s a short list of the tools I use. If you feel like there are better ones or any questions definitely let me know!&lt;/p&gt;
&lt;/div&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
  </channel>
</rss>
