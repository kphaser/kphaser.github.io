<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Wong</title>
    <link>/tags/kaggle/index.xml</link>
    <description>Recent content on Kevin Wong</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Kevin Wong</copyright>
    <atom:link href="/tags/kaggle/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Predicting Ames House Prices</title>
      <link>/post/predicting-ames-house-prices/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-ames-house-prices/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem we are trying to solve here is to build models to predict house prices, given the Ames Housing dataset, with high degree of predictive accuracy. The problem does not call for specific algorithms or techniques to be used. Just know that machine learning is a street brawl.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;significance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Significance&lt;/h3&gt;
&lt;p&gt;The goal of the problem was to utilize any and all machine learning tools or time series forecasting techniques to make the best possible prediction of house prices. This is an interesting problem because most people will eventually buy/sell a home. This problem allows us, as data scientists, to learn more about the housing market and helps with making more informed decisions.&lt;/p&gt;
&lt;p&gt;This project encouraged getting our hands dirty to clean, transform, and engineer features that enabled better predictions. This problem also necessitated learning popular algorithms and tools. The use of Kaggle as a platform for data science competition helped motivate us to keep improving.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Description&lt;/em&gt;&lt;br /&gt;
The Ames Housing dataset was retrieved from &lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data&lt;/a&gt;. The dataset represents residential properties in Ames, Iowa from 2006 to 2010. There is a train and a test file. The train file has 1460 observations and the test file has 1459 observations. Both datasets contain 79 explanatory variables composed of 46 categorical and 33 continuous variables that describe house features such as neighborhood, square footage, number of full bathrooms, and many more. The train file contains a response variable column, &lt;strong&gt;SalePrice&lt;/strong&gt;, which is what we will predict in the test set. There is also a unique ID for each house sold, but were not used in fitting the models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Exploratory Data Analysis&lt;/em&gt;&lt;br /&gt;
Below is a historgram of the SalePrice. Notice that the SalePrice is heavily skewed to the right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/saleprice-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/saleprice-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A log transformation was made to normalize the variable. This would allow algorithms such as linear regression, which rely on the assumption of linear relationships, to make better predictions.&lt;/p&gt;
&lt;p&gt;Next we examined some boxplots of certain variables against SalePrice. These help to confirm our understanding of what may affect a home’s sale price.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/neighborhoods-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that certain neighborhoods sell for more than others. The distribution of prices make sense here as there should be varying prices with certain neighborhoods clearly valued higher.&lt;/p&gt;
&lt;p&gt;Below are several more boxplots that indicate homes with more full bathrooms, finished garages, and central airconditioning tend to yield higher sale prices which is common sense.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/boxplots-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can take a look at some numeric variables and their correlations to one another. Sometimes highly correlated variables may need to be removed. &lt;img src=&#34;#####../content/post/predicting-ames-house-prices_files/figure-html/corrplot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above correlation matrix shows some variables that are highly correlated such as GrLivArea and TotRmsAbvGrd at 0.8 and GarageYrBlt and YearBuilt at 0.8. These variables were not be removed and were kept in the dataset as some of the models are fairly robust and correlated variables will not hinder predictive performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Preprocessing&lt;/em&gt;&lt;br /&gt;
The data preprocessing step proved to be the most crucial in this project. The train and test datasets were combined to keep the level of factors equal between the two sets so problems were not run into when predicting with unseen factor levels.&lt;/p&gt;
&lt;p&gt;In order to reduce the dimensionality of the dataset, the &lt;em&gt;nearZeroVar()&lt;/em&gt; function from the &lt;em&gt;caret&lt;/em&gt; package was used to remove variables that either have very few unique values. The purpose of removing these variables is because they do not add any additional information.&lt;/p&gt;
&lt;p&gt;All numeric variables that had a skewness of over 0.75 were transformed using the log function. The categorical variables were one hot encoded into binary variables which means that each factor level was given it’s own variable with values 0 or 1 representing no presence or presence of that feature.&lt;/p&gt;
&lt;p&gt;Certain variables contained many missing values. Any variables with over 20% missing were removed from the dataset. The remaining missing values were imputed using the &lt;em&gt;mice&lt;/em&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-of-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type of Models&lt;/h3&gt;
&lt;p&gt;The following are five models developed to predict house prices. These models were chosen because they showcase varying complexity and also have proven to perform well on many Kaggle problems.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;LASSO&lt;/strong&gt;&lt;br /&gt;
The LASSO, also known as Least Absolute Shrinkage and Selection Operator), is a regression model that does variable selection and regularization. The LASSO model uses a parameter that penalizes fitting too many variables. It allows the shrinkage of variable coefficients to 0, which essentially results in those variables having no effect in the model, thereby reducing dimensionality. Since there are quite a few explanatory variables, reducing the number of variables may increase interpretability and prediction accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;br /&gt;
A more advanced model, the random forest uses multiple decision trees and gives the mean prediction of each tree. This is somewhat of a black box approach as the random forest mechanism is not very clear as it will give model results, but lack information on coefficients which is something we normally get from an output of a regression model. However, the random forest model is a great general purpose algorithm that has potential to make quite accurate predictions. Random forests can be quite robusts against outliers and do not require assumptions of normality.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GBM&lt;/strong&gt; Gradient boosting models are one of the most popular algorithms on Kaggle. A variant of GBMs known as the XGBoost has been a clear favorite for many recent competitions. The algorithm works well right out of the box. It is a type of ensemble model, like the random forest, where multiple decision trees are used and optimized over some cost function. The popularity and ability to score well in competition are reasons enough to use this type of model for house price prediction problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt;&lt;br /&gt;
Neural nets have recently been popularized in the media especially for their use in deep learning. Neural networks mimic the function of a human brain. There are multiple layers containing neurons. Inputs into each layer become the output for the subsequent layer until there are no additional layers. Neural networks are useful in this problem because they can take complex datasets and find patterns. This model can be useful when dealing with mixed variable type datasets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ensemble Model/Model Averaging&lt;/strong&gt;&lt;br /&gt;
The last type of model used was the ensemble model, specifically model averaging. Several models are created that perform relaively well on the problem and the predictions are averaged. In this case, all the models above were used to create an average prediction. The reason for using this model was that the recent Kaggle winners used ensemble models. It is not surprising that most winning submissions are ensembles because one can leverage the prediction power of several different models to obtain the highest prediction accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;formulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formulation&lt;/h3&gt;
&lt;p&gt;After processing the dataset, the training was separated into training and validation sets using a 75/25 split. In this way, models were fit on the training and evaluated or tuned on validation.&lt;/p&gt;
&lt;p&gt;The models were implemented using the &lt;em&gt;caret&lt;/em&gt; and &lt;em&gt;h2o&lt;/em&gt; packages. The caret package was used primarily for parameter tuning. The package allows grid search where you specify a data frame of parameter values. It also works with hundreds of popular machine learning algorithms. The models were fit using repeated 5-fold cross-validation. An example of my usage is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitControl &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;,
                           number=5,
                           repeats=5,
                           verboseIter=FALSE)


set.seed(123)  # for reproducibility
lasso_mod &amp;lt;- train(x=x_train,y=y_train,
                   method=&amp;quot;glmnet&amp;quot;,
                   metric=&amp;quot;RMSE&amp;quot;,
                   maximize=FALSE,
                   trControl=fitControl,
                   tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                        lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                                 0.00075,0.0005,0.0001)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Models can take fairly long to run even with a dataset of this size (2919 total observations and 80+ variables). I discovered the &lt;em&gt;h2o&lt;/em&gt; package which allows R to run in-memory, in distributed fashion. This sped up many models that I had already built by at least 20x running on my local machine. Once you load the h2o package and initiate the cluster, you can fit a model like a neural network very fast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dl_fit &amp;lt;- h2o.deeplearning(x = x,
                            y = z,
                            training_frame = train_h2o,
                            model_id = &amp;quot;dl_fit2&amp;quot;,
                            # validation_frame = valid_h2o,  # only used if stopping_rounds &amp;gt; 0
                            epochs = 20,
                            hidden= c(10,10),
                            stopping_rounds = 0,  # disable early stopping
                            seed = 1)

dl_perf &amp;lt;- h2o.performance(model = dl_fit2, newdata = valid_h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the models were iterated over several variations of parameters and data preprocessing situations in order to obtain the best possible predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Performance&lt;/h3&gt;
&lt;p&gt;Each model was compared using the Log RMSE on a validation set. The Kaggle public score shows the results on the public test set. As of writing, the competition has not ended, so there are no results on the private heldout test set.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Cross Validation Log RMSE&lt;/th&gt;
&lt;th&gt;Kaggle Public Log RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;LASSO&lt;/td&gt;
&lt;td&gt;0.12783&lt;/td&gt;
&lt;td&gt;0.12390&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;0.12567&lt;/td&gt;
&lt;td&gt;0.14043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;GBM&lt;/td&gt;
&lt;td&gt;0.12382&lt;/td&gt;
&lt;td&gt;0.13733&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Neural Network&lt;/td&gt;
&lt;td&gt;0.14334&lt;/td&gt;
&lt;td&gt;0.16168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Ensemble (GLM+GBM)&lt;/td&gt;
&lt;td&gt;0.10757&lt;/td&gt;
&lt;td&gt;0.12523&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model that performed the best was the LASSO model. It did not have the best cross validation score, but the best public leaderboard score on Kaggle. The LASSO model is more easily interpretable model than the others since it reduces the dimensionality of the dataset by shrinking variables to 0. It allows for a much more manageable set of variables to work with. It makes sense that the LASSO model performed well because many variable appeared to have some linear relationship with the reponse variable, SalePrice.&lt;/p&gt;
&lt;p&gt;The next best model was the ensemble model. By leveraging multiple models, the ensemble model was able to obtain the pretty good prediction accuracy. However, brute force and adding multiple models and averaging their predictions did not prove to be the best model. Surprisingly, the Neural Network model did not perform well at all. Even with parameter tuning, it never was able to beat any of the other models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;p&gt;The Ensemble model has some clear limitations. It is not at all easily interpretable despite yielding high prediction accuracy. The Ensemble model requires brute forcing multiple models which can make things complicated fast if you are not careful. It also heavily relies on the models that go into it.&lt;/p&gt;
&lt;p&gt;The Random Forest, GBM, and Neural Network are fairly robust models that can work on a variety of classification and regression problems. Much of the limitaions of these models relies on the tuning of the parameters and the inputs to the model. The same goes for the LASSO model where one can tune the lambda values. The LASSO is more reliant on the assumption that the explanatory variables are linearly related to the response.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Future Work&lt;/h3&gt;
&lt;p&gt;Some future considerations for this problem may be to perform more feature engineering. There are likely other variables that can provide more information. The models can be further developed through more hyper-parameterization and perhaps running them in a distributed computing environment.&lt;/p&gt;
&lt;p&gt;The goal in developing these models is to be able to better understand and to apply them to other real world datasets. I am interested into taking some of these models to more complex problems like image classification and building some interactive applications around these models. My hope is to be able to showcase this project and other future similar projects to employers/clients. This problem has also inspired me to compete in other Kaggle competitions using models developed here.&lt;/p&gt;
&lt;/div&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
  </channel>
</rss>
